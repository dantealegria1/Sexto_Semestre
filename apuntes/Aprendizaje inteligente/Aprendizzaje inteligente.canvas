{
	"nodes":[
		{"id":"3f5d0f1deabce6a6","type":"group","x":-589,"y":-985,"width":1253,"height":1477,"color":"1","label":"Algoritmo con Perceptron"},
		{"id":"8a2d17f1126933a9","type":"text","text":"\nEl primer paso es crear un arrreglo con los datos que queremos usar\n``` python\npersonas = np.array([[0.3,0.4], [0.4, 0.3],\n                     [0.3,0.2], [0.4, 0.1],\n                     [0.5,0.4], [0.4, 0.8],\n                     [0.6,0.8], [0.5, 0.6],\n                     [0.7,0.6], [0.8, 0.5]])\n```","x":-497,"y":-965,"width":459,"height":262},
		{"id":"cf10eb7e11f383d7","type":"text","text":"Despues lo que se hace es crear un vector con la salida esperada\n``` python\nclases = np.array([0,0,0,0,0,1,1,1,1,1])\n```","x":79,"y":-940,"width":430,"height":147},
		{"id":"a6aa4d47dd772525","type":"text","text":"Después viene usar la función de activación\n\n```python \n# w1*x1 + w2*x2 +...+ wn*xn\ndef activacion(pesos,x, b):\n    z = pesos * x\n    if z.sum() + b > 0:\n        return 1\n    else:\n        return 0\n```\n La función de activación sirve para evaluar en este caso si se acepta o no. Es el peso por x1 + el peso 2 por x2 y si es mayor que theta pues saca uno","x":42,"y":-691,"width":504,"height":380},
		{"id":"c4b1d85bdf5e1840","type":"text","text":"Luego viene asignar los pesos y el bias \n```python\npesos = np.random.uniform(-1,1, size=2)\nb = np.random.uniform(-1,1)\npesos, b, activacion(pesos,[0.8,0.5],b)\n```\n\nLos valores del Bias y de los pesos son aleatorios entre -1 y 1\n\n(array([-0.81068785, -0.8346994 ]), 0.8358746797610048, 0)\n\nNos muestra el arreglo de los pesos, el Bias que se genero y el resultado, en este caso dice que se denegó ","x":-541,"y":-702,"width":468,"height":357},
		{"id":"e157b7d17e7b42da","type":"text","text":"Ahora vamos a entrenar al algoritmo del perceptron\n```python \npesos = np.random.uniform(-1,1, size=2)\nb = np.random.uniform(-1,1)\ntasa_de_aprendizaje = 0.01\nepocas = 100\n```\n","x":-568,"y":-232,"width":400,"height":202},
		{"id":"3cc7e431b6794795","type":"text","text":"Este es el ciclo de aprendizaje \n```python\nfor epoca in range(epocas):\n    error_total = 0\n    for i in range(len(personas)):\n        prediccion = activacion(pesos, personas[i], b)\n        error = clases[i] - prediccion\n        error_total += error**2\n        pesos[0] += tasa_de_aprendizaje * personas[i][0] * error\n        pesos[1] += tasa_de_aprendizaje * personas[i][1] * error\n        b += tasa_de_aprendizaje * error\n    print(error_total, end=\" \")\n```\n\n- Primero el Ciclo para que se detenga hasta el termino de las épocas. Creo una variable llamada error total = 0\n- Creo un Segundo ciclo que dure lo de nuestras personas\n- Predigo los valores con la función de activación dependiendo de la persona\n- El error es el valor que debería de dar (que esta en nuestro vector clases) - nuestra predicción \n- El error total es el error total + el error al cuadrado (Esto para que no de negativo sen caso de que de 1)\n- El peso en mi posición 0 (acuérdate que pesos es un vector de 2 valores) es igual al peso + mi tasa de aprendizaje * el primer valor de esa persona * mi error\n- El peso en mi posición 1 es lo mismo que en la 0, solo cambia que ahora uso el segundo valor de las personas\n- Acomodo el Bias para que sea igual a el Bias + la tasa de aprendizaje * el error \n\n\n","x":-83,"y":-297,"width":736,"height":651},
		{"id":"a748e9e4162edc50","type":"text","text":"En teoria si ahora uso el algoritmo deberia predecir correctamente \n``` python  \npesos, b, activacion(pesos,[0.5,0.8],b)\n```\n(array([-0.09555605,  0.51173695]), -0.17887316170739223, 1)\n","x":-594,"y":58,"width":451,"height":226},
		{"id":"df6a88702834f2e4","type":"file","file":"Pasted image 20240223105623.png","x":1213,"y":-922,"width":400,"height":257},
		{"id":"50b2fc526f5d5fba","type":"file","file":"Pasted image 20240223105642.png","x":2001,"y":-828,"width":400,"height":186}
	],
	"edges":[
		{"id":"224d5b71d01042ba","fromNode":"cf10eb7e11f383d7","fromSide":"bottom","toNode":"a6aa4d47dd772525","toSide":"top"},
		{"id":"3eb7ef3f9fde8c06","fromNode":"8a2d17f1126933a9","fromSide":"right","toNode":"cf10eb7e11f383d7","toSide":"left"},
		{"id":"609db9de5d6af791","fromNode":"a6aa4d47dd772525","fromSide":"left","toNode":"c4b1d85bdf5e1840","toSide":"right"},
		{"id":"330c60205b63752e","fromNode":"c4b1d85bdf5e1840","fromSide":"bottom","toNode":"e157b7d17e7b42da","toSide":"top"},
		{"id":"c652312ff108f7aa","fromNode":"e157b7d17e7b42da","fromSide":"right","toNode":"3cc7e431b6794795","toSide":"left"},
		{"id":"e95335458e6d40fe","fromNode":"3cc7e431b6794795","fromSide":"left","toNode":"a748e9e4162edc50","toSide":"right"}
	]
}